# -*- coding: utf-8 -*-
"""Parameter-Efficient Supervised Fine-Tuning of LLaMA  3.2 (3B) on a Medical Chain-of-Thought Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7CFBu8Px1mDkxk47uGRVG78iQmKDRkA

**Step 1**

import the libraries
"""

!pip install unsloth wandb datasets rouge-score

!pip install transformers
!pip install accelerate
!pip install bitsandbytes
!pip install peft
!pip install trl
!pip install sentencepiece
!pip install scipy

"""**Step 2**

in this i import weight and biases
"""

import wandb
wandb.login()

"""**Step 3**

Load the data set import from hugging face
"""

from datasets import load_dataset
dataset = load_dataset("FreedomIntelligence/medical-o1-reasoning-SFT", 'en')
train_data = dataset['train'].select(range(100, len(dataset['train'])))
val_data = dataset['train'].select(range(0, 100))

"""
**Step 4**

*   FastLanguageModel is a tool from the Unsloth library that allows you to load
LLaMA models efficiently for training or inference.
*  model_name = "unsloth/Llama-3.2-3B" tells the code to load the pre-trained LLaMA 3.2 (3B) model hosted on Hugging Face.


"""

from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B",
    load_in_4bit = True,
    max_seq_length = 2048,
    dtype = None,
    device_map = "auto",
)

"""**Step 5**

*   parameter-efficient fine-tuning (PEFT) to the loaded model using LoRA, allowing
it to fine-tune only a small subset of weights to save memory and speed up training.


"""

model = FastLanguageModel.get_peft_model(model)

"""**Step 6**

*  medical question-answer pairs with tags, tokenizes them, and creates labels for model training.


"""

def format_input(example):
    text = f"<think>{example['Question']}</think> <response>{example['Complex_CoT']}</response>"
    # Remove padding='longest' from here
    tokenized = tokenizer(text, truncation=True)
    input_ids = tokenized["input_ids"]
    attention_mask = tokenized["attention_mask"]

    # Create labels, ignore padding tokens (-100)
    # Labels should have the same length as input_ids
    labels = [tid if tid != tokenizer.pad_token_id else -100 for tid in input_ids]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
    }

train_data = train_data.map(format_input)
val_data = val_data.map(format_input)

"""**Step 8**

*   collate_batch function pads input IDs, attention masks, and labels in a batch to the same length for efficient model training.


"""

def collate_batch(batch):
    input_ids = [example["input_ids"] for example in batch]
    attention_mask = [example["attention_mask"] for example in batch]
    labels = [example["labels"] for example in batch]

    return {
        "input_ids": torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id),
        "attention_mask": torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0),
        "labels": torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100),
    }

# #def collate_batch(batch):
#  #   texts = [example["text"] for example in batch]
#   #  return tokenizer(
#    #     texts,
#         return_tensors="pt",
#         padding=True,
#         truncation=True,
#         max_length=512,
#     )
def collate_batch(batch):
    texts = [example["Question"] for example in batch]
    encodings = tokenizer(
        texts,
        padding=True,          # <--- Ensures equal length
        truncation=True,       # <--- Cuts off very long inputs
        return_tensors="pt"    # <--- Returns PyTorch tensors
    )
    # Dummy labels if you're doing language modeling or a placeholder for now
    labels = encodings["input_ids"].clone()  # Adjust as needed for your task
    encodings["labels"] = labels
    return encodings

"""**Step 9**

*   CoTDataset class prepares tokenized input data (input IDs, attention masks, and labels) for training by converting it into a PyTorch-compatible dataset format


"""

from torch.utils.data import Dataset, DataLoader
import torch

class CoTDataset(Dataset):
    def __init__(self, data):
        self.input_ids = data["input_ids"]
        self.attention_mask = data["attention_mask"]
        self.labels = data["labels"]

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": torch.tensor(self.input_ids[idx]),
            "attention_mask": torch.tensor(self.attention_mask[idx]),
            "labels": torch.tensor(self.labels[idx])
        }
# class CoTDataset(Dataset):
#     def __init__(self, data):
#         self.data = data  # This is a pandas DataFrame

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, idx):
#         return {"Question": self.data.iloc[idx]["Question"]}

# train_loader = DataLoader(CoTDataset(train_data), batch_size=1, shuffle=True, collate_fn=collate_batch)
# val_loader = DataLoader(CoTDataset(val_data), batch_size=1, collate_fn=collate_batch)
train_loader = DataLoader(CoTDataset(train_data), batch_size=2, shuffle=True, collate_fn=collate_batch)
val_loader = DataLoader(CoTDataset(val_data), batch_size=2, collate_fn=collate_batch)

"""**Step 10**

*   PyTorch DataLoaders for training and validation using a sequence-to-sequence data collator that dynamically pads the input data.


"""

from transformers import DataCollatorForSeq2Seq

collate_fn = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="pt")
train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_data, batch_size=2, collate_fn=collate_fn)

"""**Step 11**

*   Weights & Biases tracking for the project and sets up the AdamW optimizer with a learning rate of 2e-4 for model training."


"""

wandb.init(project="llama3-medical-finetune")
from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=2e-4)

device = torch.device("cuda" )
model.to(device)
model.train()

"""**Step 12**

*  Code trains the model for 3 epochs, computes loss on each batch, updates weights, and logs average training loss per epoch to Weights & Biases.

"""

for epoch in range(3):
    total_loss = 0
    print(f" Starting epoch {epoch+1}")
    for i, batch in enumerate(train_loader):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.item()
        if i % 10 == 0:
            print(f"Batch {i+1}: Loss = {loss.item():.4f}")

    avg_loss = total_loss / len(train_loader)
    wandb.log({"epoch": epoch, "train_loss": avg_loss})
    print(f"Epoch {epoch+1} complete | Avg Loss: {avg_loss:.4f}")

save_path = "llama3_medical_finetuned"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f" Model & tokenizer saved at: {save_path}")